{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path = r\"datasets\\test.tsv\"\n",
    "test_path = r\"datasets\\train.tsv\"\n",
    "valid_path = r\"datasets\\valid.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_path, sep='\\t', header=None)\n",
    "test_data = pd.read_csv(test_path, sep='\\t', header=None)\n",
    "valid_data = pd.read_csv(valid_path, sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \n",
    "    \"job_title\", \"state_info\", \"party_affiliation\", \n",
    "    \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \n",
    "    \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
    "]\n",
    "\n",
    "train_data.columns = columns\n",
    "test_data.columns = columns\n",
    "valid_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset=['statement'])\n",
    "test_data = test_data.dropna(subset=['statement'])\n",
    "valid_data = valid_data.dropna(subset=['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Data Summary Statistics:\n",
      "\n",
      "Test Data Summary Statistics:\n",
      "\n",
      "Validation Data Summary Statistics:\n",
      "\n",
      "Train Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1267 entries, 0 to 1266\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   id                    1267 non-null   object\n",
      " 1   label                 1267 non-null   object\n",
      " 2   statement             1267 non-null   object\n",
      " 3   subject               1267 non-null   object\n",
      " 4   speaker               1267 non-null   object\n",
      " 5   job_title             942 non-null    object\n",
      " 6   state_info            1005 non-null   object\n",
      " 7   party_affiliation     1267 non-null   object\n",
      " 8   barely_true_counts    1267 non-null   int64 \n",
      " 9   false_counts          1267 non-null   int64 \n",
      " 10  half_true_counts      1267 non-null   int64 \n",
      " 11  mostly_true_counts    1267 non-null   int64 \n",
      " 12  pants_on_fire_counts  1267 non-null   int64 \n",
      " 13  context               1250 non-null   object\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 138.7+ KB\n",
      "\n",
      "Test Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10240 entries, 0 to 10239\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    10240 non-null  object \n",
      " 1   label                 10240 non-null  object \n",
      " 2   statement             10240 non-null  object \n",
      " 3   subject               10238 non-null  object \n",
      " 4   speaker               10238 non-null  object \n",
      " 5   job_title             7342 non-null   object \n",
      " 6   state_info            8030 non-null   object \n",
      " 7   party_affiliation     10238 non-null  object \n",
      " 8   barely_true_counts    10238 non-null  float64\n",
      " 9   false_counts          10238 non-null  float64\n",
      " 10  half_true_counts      10238 non-null  float64\n",
      " 11  mostly_true_counts    10238 non-null  float64\n",
      " 12  pants_on_fire_counts  10238 non-null  float64\n",
      " 13  context               10138 non-null  object \n",
      "dtypes: float64(5), object(9)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "Validation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1284 entries, 0 to 1283\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   id                    1284 non-null   object\n",
      " 1   label                 1284 non-null   object\n",
      " 2   statement             1284 non-null   object\n",
      " 3   subject               1284 non-null   object\n",
      " 4   speaker               1284 non-null   object\n",
      " 5   job_title             939 non-null    object\n",
      " 6   state_info            1005 non-null   object\n",
      " 7   party_affiliation     1284 non-null   object\n",
      " 8   barely_true_counts    1284 non-null   int64 \n",
      " 9   false_counts          1284 non-null   int64 \n",
      " 10  half_true_counts      1284 non-null   int64 \n",
      " 11  mostly_true_counts    1284 non-null   int64 \n",
      " 12  pants_on_fire_counts  1284 non-null   int64 \n",
      " 13  context               1272 non-null   object\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 140.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain Data Summary Statistics:\")\n",
    "train_data.describe()\n",
    "\n",
    "print(\"\\nTest Data Summary Statistics:\")\n",
    "test_data.describe()\n",
    "\n",
    "print(\"\\nValidation Data Summary Statistics:\")\n",
    "valid_data.describe()\n",
    "\n",
    "\n",
    "print(\"\\nTrain Data Info:\")\n",
    "train_data.info()\n",
    "\n",
    "print(\"\\nTest Data Info:\")\n",
    "test_data.info()\n",
    "\n",
    "print(\"\\nValidation Data Info:\")\n",
    "valid_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "half-true      265\n",
      "false          249\n",
      "mostly-true    241\n",
      "barely-true    212\n",
      "true           208\n",
      "pants-fire      92\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'half-true': 0.7968553459119497, 'false': 0.8480589022757697, 'mostly-true': 0.876210235131397, 'barely-true': 0.9960691823899371, 'true': 1.015224358974359, 'pants-fire': 2.295289855072464}\n"
     ]
    }
   ],
   "source": [
    "classes = np.array([\"half-true\", \"false\", \"mostly-true\", \"barely-true\", \"true\", \"pants-fire\"])\n",
    "\n",
    "# compute class weights because the above classes are imbalanced according to the value counts from the above cell output \n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_data['label']\n",
    ")\n",
    "\n",
    "# weights = weights / np.sum(weights) # normalized the weights to sum to 1 \n",
    "print(dict(zip(classes, weights)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\U765123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\U765123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267, 4078) (10240, 4078) (1284, 4078)\n",
      "(1267,) (10240,) (1284,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words and word.isalnum()]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "\n",
    "train_data['statement'] = train_data['statement'].apply(preprocess_text)\n",
    "test_data['statement'] = test_data['statement'].apply(preprocess_text)\n",
    "valid_data['statement'] = valid_data['statement'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# tf-idf vectorization\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(train_data['statement'])\n",
    "\n",
    "X_train = vectorizer.transform(train_data['statement']).toarray()\n",
    "X_test = vectorizer.transform(test_data['statement']).toarray()\n",
    "X_valid = vectorizer.transform(valid_data['statement']).toarray()\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "y_valid = valid_data['label']\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_valid.shape)\n",
    "print(y_train.shape, y_test.shape, y_valid.shape)\n",
    "\n",
    "# save the preprocessed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2119140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.20      0.20      0.20      1654\n",
      "       false       0.23      0.21      0.22      1995\n",
      "   half-true       0.23      0.22      0.22      2114\n",
      " mostly-true       0.23      0.24      0.23      1962\n",
      "  pants-fire       0.14      0.14      0.14       839\n",
      "        true       0.20      0.22      0.21      1676\n",
      "\n",
      "    accuracy                           0.21     10240\n",
      "   macro avg       0.20      0.20      0.20     10240\n",
      "weighted avg       0.21      0.21      0.21     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Precision is about how accurate the model's positive predictions are\n",
    "- Recall is about how well the model identifies positive cases\n",
    "- F1-Score balances precision and recall\n",
    "- Support shows how many instances of each class exist in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train = train_data['statement']\n",
    "y_train = train_data['label']\n",
    "X_test = test_data['statement']\n",
    "y_test = test_data['label']\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21572265625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.20      0.15      0.18      1654\n",
      "       false       0.23      0.28      0.25      1995\n",
      "   half-true       0.21      0.32      0.25      2114\n",
      " mostly-true       0.23      0.26      0.24      1962\n",
      "  pants-fire       0.67      0.00      0.00       839\n",
      "        true       0.20      0.12      0.15      1676\n",
      "\n",
      "    accuracy                           0.22     10240\n",
      "   macro avg       0.29      0.19      0.18     10240\n",
      "weighted avg       0.25      0.22      0.20     10240\n",
      "\n",
      "[[256 413 540 327   0 118]\n",
      " [306 554 573 361   0 201]\n",
      " [250 480 681 498   1 204]\n",
      " [176 394 684 514   0 194]\n",
      " [138 222 275 126   2  76]\n",
      " [125 394 506 449   0 202]]\n",
      "0.21572265625\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- In the confusion matrix, the rows represent the actual classes and the columns represent the predicted classes.\n",
    "- The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier.\n",
    "- The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\n",
    "\n",
    "The results are not very good, but this is expected since we are using a simple logistic regression model with a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2201171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.20      0.14      0.17      1654\n",
      "       false       0.23      0.28      0.25      1995\n",
      "   half-true       0.22      0.37      0.27      2114\n",
      " mostly-true       0.23      0.26      0.24      1962\n",
      "  pants-fire       0.00      0.00      0.00       839\n",
      "        true       0.20      0.10      0.13      1676\n",
      "\n",
      "    accuracy                           0.22     10240\n",
      "   macro avg       0.18      0.19      0.18     10240\n",
      "weighted avg       0.20      0.22      0.20     10240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\U765123\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\U765123\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\U765123\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with different hyperparameters\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.20      0.09      0.13      1654\n",
      "       false       0.23      0.23      0.23      1995\n",
      "   half-true       0.22      0.53      0.31      2114\n",
      " mostly-true       0.22      0.21      0.21      1962\n",
      "  pants-fire       0.00      0.00      0.00       839\n",
      "        true       0.22      0.05      0.09      1676\n",
      "\n",
      "    accuracy                           0.22     10240\n",
      "   macro avg       0.18      0.19      0.16     10240\n",
      "weighted avg       0.20      0.22      0.18     10240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\U765123\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\U765123\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\U765123\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# the results are not better than the previous model, so we will stick with the previous model\n",
    "\n",
    "# retraining the model with these new features\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.21      0.21      0.21      1654\n",
      "       false       0.22      0.24      0.23      1995\n",
      "   half-true       0.22      0.24      0.23      2114\n",
      " mostly-true       0.22      0.25      0.24      1962\n",
      "  pants-fire       0.14      0.05      0.07       839\n",
      "        true       0.21      0.18      0.19      1676\n",
      "\n",
      "    accuracy                           0.21     10240\n",
      "   macro avg       0.20      0.20      0.20     10240\n",
      "weighted avg       0.21      0.21      0.21     10240\n",
      "\n",
      "[[353 366 361 332  55 187]\n",
      " [399 487 409 363  66 271]\n",
      " [326 420 515 490  53 310]\n",
      " [264 372 485 500  41 300]\n",
      " [186 202 196 116  40  99]\n",
      " [193 356 361 441  21 304]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', class_weight='balanced')\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
